#!/usr/bin/env python
# coding: utf-8

# # Week 2 Programming Assignment 
# 
# Remark: 
# 
# Please upload your solutions of this assignment to Canvas with a file named "Programming_Assignment_2 _yourname.ipynb" before 11:59pm May 30.

# =================================================================================================================

# ### **Problem 1 (4 pt).** Use stochastic gradient descent method to train MNIST with the logisitc regression model to achieve at least 92% test accuracy. Print the results with the following format:
# 
#    "Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$"
# 
# where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch.

# In[1]:


# write your code for solving probelm 1 in this cell
# Hint: you can tune the number of epoches, learning rate and mini-batch size.


# =================================================================================================================

# ### **Problem 2 (6 pts).** Extract the subset of data which are labeled with 0,1,3,4,7 from MNIST dataset. Use both full gradient descent method and stochastic gradient descent method to train this subset with the logisitc regression model to achieve the training and test accuracy as high as possible. Print the results with the following format:
# 
# * For full gradient descent method, print:
# 
#     "Full gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$"
# 
# 
# * For stochastic gradient descent method, print:
# 
#     "Stochastic gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$"
# 
# where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch.

# In[2]:


# write your code for solving probelm 2 in this cell
# Hint: you can tune the number of epoches, learning rate and mini-batch size.


# =================================================================================================================
